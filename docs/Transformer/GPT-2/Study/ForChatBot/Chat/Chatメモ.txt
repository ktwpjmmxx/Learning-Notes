**Google Colab Hugging Face GPT-2ã«ã‚ˆã‚‹é«˜å“è³ªãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆé–‹ç™º

âš«ï¸ç’°å¢ƒè¨­å®šã¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

ã€åŸºæœ¬ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã€‘

!pip install transformers>=4.21.0
!pip install torch>=1.12.0  
!pip install tokenizers>=0.12.0
!pip install datasets
!pip install gradio
!pip install wandb

transformers: Hugging Faceã®ä¸»åŠ›ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã€‚GPT-2ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã¨æ¨è«–ã«å¿…è¦
torch: PyTorchãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€‚ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè¡ŒåŸºç›¤
tokenizers: é«˜é€Ÿãªãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
gradio: Web UIã‚’ç°¡å˜ã«æ§‹ç¯‰ã§ãã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
wandb: æ©Ÿæ¢°å­¦ç¿’ã®å®Ÿé¨“è¿½è·¡ãƒ»å¯è¦–åŒ–ãƒ„ãƒ¼ãƒ«


ã€GPT-2ãƒ¢ãƒ‡ãƒ«é¸æŠã®æŒ‡é‡ã€‘

model_options = {
    'gpt2': {
        'parameters': '117M',
        'memory_usage': '~500MB',
        'inference_speed': 'Fast',
        'quality': 'Good for simple conversations'
    },
    'gpt2-medium': {
        'parameters': '345M', 
        'memory_usage': '~1.3GB',
        'inference_speed': 'Moderate',
        'quality': 'Better coherence and knowledge'
    }
}


ã“ã®è¾æ›¸æ§‹é€ ã«ã‚ˆã‚Šã€ç”¨é€”ã«å¿œã˜ã¦ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã‚’é¸æŠã§ãã‚‹ã€‚
gpt2 (117M): ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚°ãƒ»ãƒ†ã‚¹ãƒˆç”¨
gpt2-medium (345M): å®Ÿç”¨çš„ãªãƒãƒ©ãƒ³ã‚¹
gpt2-large (762M): é«˜å“è³ªãŒå¿…è¦ãªå ´åˆ

ãã‚Œãã‚Œã®ãƒ¢ãƒ‡ãƒ«ã«ä»¥ä¸‹ãŒæ•´ç†ã•ã‚Œã¦ã„ã‚‹ã€‚

parametersï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ï¼‰
memory_usageï¼ˆæ¨å®šãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼‰
inference_speedï¼ˆæ¨è«–é€Ÿåº¦ï¼‰
qualityï¼ˆä¼šè©±ã®è³ªï¼‰


ã€ä¼šè©±ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…ã€‘

ConversationManagerã‚¯ãƒ©ã‚¹

class ConversationManager:
    def __init__(self, max_history=10):
        self.conversation_history = []
        self.max_history = max_history
        self.persona = ""
        self.current_topic = None
    
    def add_message(self, role, message):
        self.conversation_history.append({
            'role': role,
            'message': message, 
            'timestamp': time.time()
        })
        
        if len(self.conversation_history) > self.max_history:
            self.conversation_history.pop(0)

(å±¥æ­´ç®¡ç†ã®é‡è¦æ€§)

ãƒ»ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã¯éå»ã®æ–‡è„ˆã‚’ç†è§£ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™
ãƒ»max_historyã§å¤ã„ä¼šè©±ã‚’è‡ªå‹•å‰Šé™¤ã—ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’åˆ¶å¾¡

(ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®è¨­è¨ˆ)

ãƒ»å„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¾æ›¸ã§ç®¡ç†ï¼ˆrole, message, timestampï¼‰
ãƒ»ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã«ã‚ˆã‚Šä¼šè©±ã®æ™‚ç³»åˆ—åˆ†æãŒå¯èƒ½

ğŸ”·ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰

def get_context(self):
    context = f"{self.persona}\n\n" if self.persona else ""
    for msg in self.conversation_history[-5:]:
        context += f"{msg['role']}: {msg['message']}\n"
    return context

self.persona ã«ä½•ã‹ï¼ˆä¾‹ãˆã°ã€Œã‚ãªãŸã¯å„ªã—ã„AIã§ã™ã€ã¿ãŸã„ãªè¨­å®šï¼‰ãŒå…¥ã£ã¦ã„ã‚Œã°ã€ãã‚Œã‚’å†’é ­ã«åŠ ãˆã‚‹ã€‚
ã‚‚ã—ç©ºæ–‡å­—ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—ã€‚

for msg in self.conversation_history[-5:]:
â†’å±¥æ­´ã®æœ€å¾Œã® 5 ä»¶ã ã‘ã‚’å–ã‚Šå‡ºã—ã¦ãƒ«ãƒ¼ãƒ—ã€‚
ï¼ˆ[-5:] ã£ã¦ã‚¹ãƒ©ã‚¤ã‚¹ã§ã€ãƒªã‚¹ãƒˆã®å¾Œã‚ã‹ã‚‰5å€‹ã‚’å–ã‚‹ï¼‰

æœ€çµ‚çš„ã« ã€Œãƒšãƒ«ã‚½ãƒŠè¨­å®šï¼‹ç›´è¿‘5ä»¶ã®ä¼šè©±ã€ ãŒä¸€ã¤ã®æ–‡å­—åˆ—ã¨ã—ã¦è¿”ã‚‹ã€‚
ã“ã‚Œã‚’ãƒ¢ãƒ‡ãƒ«ã«å…¥ã‚Œã‚Œã°ã€Œä»Šã¾ã§ã®ä¼šè©±ã‚’è¸ã¾ãˆã¦è¿”ç­”ã€ã§ãã‚‹ã‚ˆã†ã«ãªã‚‹ã€‚

âš«ï¸ãƒã‚¤ãƒ³ãƒˆ
1å®Ÿéš›ã«å±¥æ­´ãã®ã‚‚ã®ã¯ self.conversation_history ã«å…¨ä»¶ä¿å­˜ã•ã‚Œã¦ã‚‹
2context ã¯ãã® ä¸€éƒ¨ï¼ˆç›´è¿‘5ä»¶ï¼‰ã‚’æ•´å½¢ã—ãŸæ–‡å­—åˆ—

ç¾çŠ¶ã®ConversationManager
â†’å±¥æ­´ã‚’ç®¡ç†ã™ã‚‹ç®± â†’ self.conversation_history
å±¥æ­´ã‚’ç®¡ç†ã™ã‚‹ç®± â†’ self.conversation_history

ã€ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆæœ¬ä½“ã®å®Ÿè£…ã€‘

ğŸ”·GPT2Chatbotã‚¯ãƒ©ã‚¹ã®æ ¸å¿ƒéƒ¨åˆ†

class GPT2Chatbot:
    def __init__(self, model_name='gpt2-medium', device='cuda'):
        self.device = device
        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)
        self.model = GPT2LMHeadModel.from_pretrained(model_name).to(device)
        
        # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³è¨­å®š
        self.tokenizer.pad_token = self.tokenizer.eos_token
        self.conversation_manager = ConversationManager()
        self.response_cache = {}





























