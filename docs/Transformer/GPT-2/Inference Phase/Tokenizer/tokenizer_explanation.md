# GPT-2 Tokenizerã®ä»•çµ„ã¿ï¼šã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰/ãƒ‡ã‚³ãƒ¼ãƒ‰ ã¨BPE

## 1. BPE (Byte Pair Encoding) ã¨ã¯

BPEã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’åŠ¹ç‡çš„ã«ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã™ã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã™ã€‚

### BPEã®åŸºæœ¬åŸç†
```
æ®µéšçš„ã«ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰å˜ä½ã‚’å­¦ç¿’ï¼š
1. æ–‡å­—ãƒ¬ãƒ™ãƒ«ã‹ã‚‰é–‹å§‹
2. æœ€ã‚‚é »å‡ºã™ã‚‹ãƒšã‚¢ã‚’çµ±åˆ
3. èªå½™ã‚µã‚¤ã‚ºã«é”ã™ã‚‹ã¾ã§ç¹°ã‚Šè¿”ã—

ä¾‹ï¼š
"hello" â†’ ['h', 'e', 'l', 'l', 'o']
é »å‡ºãƒšã‚¢ "ll" ã‚’çµ±åˆ â†’ ['h', 'e', 'll', 'o']
```

### GPT-2ã§ã®BPEç‰¹å¾´
- **èªå½™ã‚µã‚¤ã‚º**: 50,257ãƒˆãƒ¼ã‚¯ãƒ³
- **æœªçŸ¥èªå¯¾å¿œ**: ã©ã‚“ãªæ–‡å­—åˆ—ã‚‚åˆ†å‰²å¯èƒ½
- **åŠ¹ç‡æ€§**: é »å‡ºå˜èªã¯1ãƒˆãƒ¼ã‚¯ãƒ³ã€ç¨€ãªå˜èªã¯è¤‡æ•°ãƒˆãƒ¼ã‚¯ãƒ³ã«åˆ†å‰²

## 2. ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰å‡¦ç†

### åŸºæœ¬çš„ãªã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
```python
text = "Hello, world!"
input_ids = tokenizer.encode(text)
print(input_ids)  # [15496, 11, 995, 0]
```

### è©³ç´°ãªã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰å‡¦ç†ãƒ•ãƒ­ãƒ¼
1. **æ­£è¦åŒ–**: ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†
2. **BPEåˆ†å‰²**: ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰å˜ä½ã«åˆ†å‰²
3. **IDå¤‰æ›**: èªå½™è¾æ›¸ã§ãƒˆãƒ¼ã‚¯ãƒ³IDã«å¤‰æ›
4. **ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ä»˜ä¸**: å¿…è¦ã«å¿œã˜ã¦é–‹å§‹/çµ‚äº†ãƒˆãƒ¼ã‚¯ãƒ³è¿½åŠ 

### ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã‚ªãƒ—ã‚·ãƒ§ãƒ³
```python
# åŸºæœ¬
tokens = tokenizer.encode("Hello world")

# ãƒ†ãƒ³ã‚½ãƒ«å½¢å¼ã§è¿”ã™
tokens = tokenizer.encode("Hello world", return_tensors="pt")

# è©³ç´°æƒ…å ±ã‚‚å–å¾—
encoded = tokenizer.encode_plus(
    "Hello world",
    return_tensors="pt",
    return_attention_mask=True,
    padding=True
)
```

## 3. ãƒ‡ã‚³ãƒ¼ãƒ‰å‡¦ç†

### åŸºæœ¬çš„ãªãƒ‡ã‚³ãƒ¼ãƒ‰
```python
token_ids = [15496, 11, 995]
text = tokenizer.decode(token_ids)
print(text)  # "Hello, world"
```

### ãƒ‡ã‚³ãƒ¼ãƒ‰å‡¦ç†ãƒ•ãƒ­ãƒ¼
1. **IDâ†’ãƒˆãƒ¼ã‚¯ãƒ³å¤‰æ›**: èªå½™è¾æ›¸ã‚’å‚ç…§
2. **BPEé€†å¤‰æ›**: ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ã‚’çµåˆ
3. **ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³å‡¦ç†**: é™¤å»ã¾ãŸã¯ä¿æŒ
4. **ãƒ†ã‚­ã‚¹ãƒˆå¾©å…ƒ**: æœ€çµ‚çš„ãªæ–‡å­—åˆ—ç”Ÿæˆ

### ãƒ‡ã‚³ãƒ¼ãƒ‰ã‚ªãƒ—ã‚·ãƒ§ãƒ³
```python
# ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‚’é™¤å»ï¼ˆæ¨å¥¨ï¼‰
text = tokenizer.decode(token_ids, skip_special_tokens=True)

# ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä¿æŒ
text = tokenizer.decode(token_ids, skip_special_tokens=False)

# ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãªã—
text = tokenizer.decode(token_ids, clean_up_tokenization_spaces=False)
```

## 4. å®Ÿéš›ã®ä¾‹ã§ç†è§£ã™ã‚‹

### ä¾‹1: å˜ç´”ãªå˜èª
```python
text = "hello"
tokens = tokenizer.tokenize(text)    # ['hello']
ids = tokenizer.encode(text)         # [31373]
decoded = tokenizer.decode(ids)      # "hello"
```

### ä¾‹2: æœªçŸ¥èªãƒ»è¤‡åˆèª
```python
text = "GPT-2ã¯ç´ æ™´ã‚‰ã—ã„"
tokens = tokenizer.tokenize(text)    # ['G', 'PT', '-', '2', 'ã¯', 'ç´ ', 'æ™´', 'ã‚‰ã—ã„']
ids = tokenizer.encode(text)         # [38, 11571, 12, 17, ...]
decoded = tokenizer.decode(ids)      # "GPT-2ã¯ç´ æ™´ã‚‰ã—ã„"
```

### ä¾‹3: ç‰¹æ®Šæ–‡å­—ãƒ»è¨˜å·
```python
text = "Hello ğŸ¤– World!"
tokens = tokenizer.tokenize(text)    # ['Hello', 'Ä ', 'ğŸ¤–', 'Ä ', 'World', '!']
# æ³¨: 'Ä 'ã¯ã‚¹ãƒšãƒ¼ã‚¹ã‚’è¡¨ã™BPEè¡¨ç¾
```

## 5. ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆå®Ÿè£…ã§ã®æ´»ç”¨

### å…¥åŠ›å‡¦ç†
```python
def prepare_input(user_text, conversation_history):
    # ä¼šè©±å±¥æ­´ã¨æ–°ã—ã„å…¥åŠ›ã‚’çµåˆ
    full_context = conversation_history + user_text
    
    # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
    input_ids = tokenizer.encode(full_context, return_tensors="pt")
    
    # é•·ã•åˆ¶é™ï¼ˆGPT-2ã¯1024ãƒˆãƒ¼ã‚¯ãƒ³ã¾ã§ï¼‰
    if input_ids.size(1) > 1000:
        input_ids = input_ids[:, -1000:]  # æœ«å°¾1000ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿ä¿æŒ
    
    return input_ids
```

### å‡ºåŠ›å‡¦ç†
```python
def process_output(generated_ids, input_length):
    # å…¥åŠ›éƒ¨åˆ†ã‚’é™¤å»ï¼ˆæ–°ã—ãç”Ÿæˆã•ã‚ŒãŸéƒ¨åˆ†ã®ã¿å–å¾—ï¼‰
    new_tokens = generated_ids[0][input_length:]
    
    # ãƒ‡ã‚³ãƒ¼ãƒ‰
    response = tokenizer.decode(new_tokens, skip_special_tokens=True)
    
    # å¾Œå‡¦ç†ï¼ˆä¸å®Œå…¨ãªæ–‡ã®é™¤å»ãªã©ï¼‰
    response = response.split('.')[0] + '.'  # æœ€åˆã®å®Œå…¨ãªæ–‡ã®ã¿
    
    return response
```

## 6. BPEã®åˆ©ç‚¹

### 1. **èªå½™å¤–å˜èªï¼ˆOOVï¼‰å•é¡Œã®è§£æ±º**
- ä»»æ„ã®æ–‡å­—åˆ—ã‚’åˆ†å‰²å¯èƒ½
- æ–°ã—ã„å˜èªã‚„å›ºæœ‰åè©ã«ã‚‚å¯¾å¿œ

### 2. **åŠ¹ç‡çš„ãªè¡¨ç¾**
- é »å‡ºå˜èªï¼š1ãƒˆãƒ¼ã‚¯ãƒ³
- ç¨€ãªå˜èªï¼šè¤‡æ•°ãƒˆãƒ¼ã‚¯ãƒ³ã«åˆ†å‰²
- ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãŒè‰¯ã„

### 3. **å¤šè¨€èªå¯¾å¿œ**
- æ–‡å­—ãƒ¬ãƒ™ãƒ«ã§å‡¦ç†ã™ã‚‹ãŸã‚å¤šè¨€èªã«å¯¾å¿œ
- è¨€èªå›ºæœ‰ã®å‰å‡¦ç†ãŒä¸è¦

## 7. æ³¨æ„ç‚¹

### ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®æŠŠæ¡
```python
text = "é•·ã„ãƒ†ã‚­ã‚¹ãƒˆã®ä¾‹..."
token_count = len(tokenizer.encode(text))
print(f"ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {token_count}")  # GPT-2ã®åˆ¶é™ï¼ˆ1024ï¼‰ã‚’ç¢ºèª
```

### ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰/ãƒ‡ã‚³ãƒ¼ãƒ‰ã®å¯é€†æ€§
```python
original = "Hello world"
encoded = tokenizer.encode(original)
decoded = tokenizer.decode(encoded)
assert original == decoded  # åŸºæœ¬çš„ã«å¯é€†ã ãŒã€ç‰¹æ®Šæ–‡å­—ã§ä¾‹å¤–ã‚ã‚Š
```

ã“ã®ä»•çµ„ã¿ã«ã‚ˆã‚Šã€GPT-2ã¯æŸ”è»Ÿã§åŠ¹ç‡çš„ãªãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã‚’å®Ÿç¾ã—ã¦ã„ã¾ã™ã€‚