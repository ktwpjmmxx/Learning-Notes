###GPT-2について

GPT(Generative Pre-Trained Transformer)

GPT-2の基本概要

# GPT-2の基本概要と内部処理

GPT-2は「デコーダーのみ」のTransformerモデルで、自然言語を自己回帰的に生成・理解するためのモデル。  
主な特徴は以下のとおり：

- **デコーダー層が複数（12層、24層、48層などモデルサイズによる）積み重なっている**  
- **マルチヘッドセルフアテンション (Multi-Head Self-Attention) を用いて文脈理解を行う**  
- **トークンごとに前のトークンだけを参照して次を予測（自己回帰）**  
- **残差接続とLayer Normalizationを駆使して学習の安定化を図る**

---

## 1. 入力トークンの埋め込み (Embedding)

入力は単語やサブワードのID列。これを学習可能な埋め込み行列でベクトルに変換。  

ここで、`N`はシーケンス長、`d`は埋め込み次元（例：768など）。

---

## 2. Transformer デコーダーブロックの処理フロー

各デコーダーブロックは以下の3つの主要構成要素を持つ。

### (1) マルチヘッドセルフアテンション (Multi-Head Self-Attention)

- 入力 `X` をクエリ (`Q`)、キー (`K`)、バリュー (`V`) に線形変換：


- スケールドドットプロダクトアテンションの計算：


ここで、`d_k`はキーの次元。  

- これを複数のヘッド（例えば12ヘッド）で計算し、ヘッドごとの結果を結合し、再度線形変換：

MultiHead(Q, K, V) = Concat(head_1, ..., head_h) W^O


- **GPT-2のセルフアテンションは未来のトークンを見ないようにマスク（causal mask）がかかっている。**

---

### (2) 残差接続 + Layer Normalization

- Attentionの出力 `A` に入力 `X` を足して残差接続を行い、その後LayerNorm：

Y = LayerNorm(X + A)


---

### (3) フィードフォワードネットワーク (Position-wise Feed-Forward Network)

- 2層の全結合ネットワーク。非線形関数（通常GeLU）を挟む：

FFN(Y) = max(0, Y W_1 + b_1) W_2 + b_2


- ここも残差接続＋LayerNormを適用：

Z = LayerNorm(Y + FFN(Y))


---

## 3. 出力

最後の層の出力 `Z` は単語分布に変換され、次トークン予測に使われる。

---

# まとめ

- **入力埋め込み → セルフアテンション（未来マスク） → 残差＋正規化 → FFN → 残差＋正規化**  
- これをN層積み重ねて深い文脈理解を獲得  
- 未来を見ない仕組みが「次の単語を順に予測する」自己回帰モデルの肝

---
